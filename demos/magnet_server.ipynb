{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAGNeT\n",
    "Welcome to MAGNeT's demo jupyter notebook. \n",
    "Here you will find a self-contained example of how to use MAGNeT for music/sound-effect generation.\n",
    "\n",
    "First, we start by initializing MAGNeT for music generation, you can choose a model from the following selection:\n",
    "1. facebook/magnet-small-10secs - a 300M non-autoregressive transformer capable of generating 10-second music conditioned on text.\n",
    "2. facebook/magnet-medium-10secs - 1.5B parameters, 10 seconds music samples.\n",
    "3. facebook/magnet-small-30secs - 300M parameters, 30 seconds music samples.\n",
    "4. facebook/magnet-medium-30secs - 1.5B parameters, 30 seconds music samples.\n",
    "\n",
    "We will use the `facebook/magnet-small-10secs` variant for the purpose of this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from audiocraft.models import MAGNeT\n",
    "import torch, torchaudio\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "from audiocraft.data.audio import audio_write\n",
    "\n",
    "model = MAGNeT.get_pretrained('facebook/magnet-small-10secs')\n",
    "\n",
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    temperature=3.0,\n",
    "    max_cfg_coef=10.0,\n",
    "    min_cfg_coef=1.0,\n",
    "    decoding_steps=[int(20 * model.lm.cfg.dataset.segment_duration // 10),  10, 10, 10],\n",
    "    span_arrangement='stride1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us configure the generation parameters. Specifically, you can control the following:\n",
    "* `use_sampling` (bool, optional): use sampling if True, else do argmax decoding. Defaults to True.\n",
    "* `top_k` (int, optional): top_k used for sampling. Defaults to 0.\n",
    "* `top_p` (float, optional): top_p used for sampling, when set to 0 top_k is used. Defaults to 0.9.\n",
    "* `temperature` (float, optional): Initial softmax temperature parameter. Defaults to 3.0.\n",
    "* `max_clsfg_coef` (float, optional): Initial coefficient used for classifier free guidance. Defaults to 10.0.\n",
    "* `min_clsfg_coef` (float, optional): Final coefficient used for classifier free guidance. Defaults to 1.0.\n",
    "* `decoding_steps` (list of n_q ints, optional): The number of iterative decoding steps, for each of the n_q RVQ codebooks.\n",
    "* `span_arrangement` (str, optional): Use either non-overlapping spans ('nonoverlap') or overlapping spans ('stride1') \n",
    "                                      in the masking scheme. \n",
    "\n",
    "When left unchanged, MAGNeT will revert to its default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can go ahead and start generating music given textual prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-conditional Generation - Music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_fade(audio, sr, duration=.1):\n",
    "    # convert to audio indices (samples)\n",
    "    length = int(duration*sr)\n",
    "    end = audio.shape[0]\n",
    "    start = end - length\n",
    "\n",
    "    # compute fade out curve\n",
    "    # linear fade\n",
    "    fade_curve = np.linspace(1.0, 0.0, length)\n",
    "\n",
    "    # apply the curve\n",
    "    audio[start:end] = audio[start:end] * fade_curve\n",
    "\n",
    "    fade_curve = np.linspace(0.0, 1.0, length)\n",
    "\n",
    "    # apply the curve\n",
    "    audio[:length] = audio[:length] * fade_curve\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP Extraction\n",
    "from madmom.features.downbeats import DBNDownBeatTrackingProcessor\n",
    "from madmom.features.downbeats import RNNDownBeatProcessor\n",
    "import os\n",
    "import soundfile as sf\n",
    "import pyrubberband as pyrb\n",
    "import librosa\n",
    "\n",
    "proc = DBNDownBeatTrackingProcessor(beats_per_bar=4, fps = 100, verbose=False)\n",
    "\n",
    "def extract_loop(file_path, desired_bpm, num_bars=2):\n",
    "    try:\n",
    "        _, sr = librosa.core.load(file_path, sr=None) # sr = None will retrieve the original sampling rate = 44100\n",
    "    except:\n",
    "        print('load file failed')\n",
    "        return None\n",
    "    try:\n",
    "        act = RNNDownBeatProcessor(verbose=False)(file_path)\n",
    "        down_beat=proc(act, verbose=False) # [..., 2] 2d-shape numpy array\n",
    "    except Exception as exp:\n",
    "        print('except happended', exp)\n",
    "        return None\n",
    "    count = 0\n",
    "    name = file_path.replace('.wav', '')\n",
    "    for i in range(8, down_beat.shape[0]):\n",
    "        if down_beat[i][1] == 1 and i + 4*num_bars < down_beat.shape[0] and down_beat[i+4*num_bars][1] == 1:\n",
    "            start_time = down_beat[i][0]\n",
    "            end_time = down_beat[i + 4*num_bars][0]\n",
    "            count += 1\n",
    "            out_path = os.path.join(\"./\", f'{name}_{count}.wav')\n",
    "            y_one_bar, _ = librosa.core.load(file_path, offset=start_time, duration = end_time - start_time, sr=None)\n",
    "            desired_duration = 60./desired_bpm * (4*num_bars)\n",
    "            y_stretch = pyrb.time_stretch(y_one_bar, sr,  (end_time - start_time) / desired_duration)\n",
    "            y_stretch = apply_fade(y_stretch, sr)\n",
    "            sf.write(out_path, y_stretch, sr)\n",
    "            return out_path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "def _loop_gen(filepath, bpm, temperature=1.0, description=\"jazzy beat\"):\n",
    "\n",
    "    prompt_waveform, prompt_sr = torchaudio.load(filepath)\n",
    "    prompt_duration = 2.0\n",
    "    prompt_waveform = prompt_waveform[..., :int(prompt_duration * prompt_sr)]\n",
    "    prompt_waveform = prompt_waveform[None, ...].repeat(batch_size, 1, 1)\n",
    "\n",
    "    desc = [description] * batch_size\n",
    "    output = model.generate_continuation(prompt_waveform, prompt_sample_rate=prompt_sr, \n",
    "                                         progress=True, return_tokens=True, descriptions=desc)\n",
    "    # output = model.generate_continuation(prompt_waveform, prompt_sample_rate=prompt_sr, descriptions=descriptions, \n",
    "                                        #  progress=True, return_tokens=True)\n",
    "    print(output[0].shape)\n",
    "    sr = model.sample_rate\n",
    "    start_pos = int(prompt_duration * sr) \n",
    "    display_audio(output[0][:,:,start_pos:], sample_rate=32000)\n",
    "\n",
    "    outpaths = []\n",
    "    for index in range(batch_size):\n",
    "        file_path = \"/mnt/c/tmp/output%d\" % index\n",
    "        start_pos = 0\n",
    "        output_data = output[0][index].cpu().squeeze()[start_pos:]\n",
    "        audio_write(file_path, output_data, sr, strategy=\"loudness\", loudness_compressor=True)\n",
    "        outpath = extract_loop(file_path + \".wav\", bpm, num_bars=2)\n",
    "        outpaths.append(outpath)\n",
    "    return outpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/madmom/features/downbeats.py:287: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  best = np.argmax(np.asarray(results)[:, 1])\n",
      "/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/madmom/features/downbeats.py:287: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  best = np.argmax(np.asarray(results)[:, 1])\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os \n",
    "import time\n",
    "from threading import Thread\n",
    "from pythonosc import dispatcher\n",
    "from pythonosc import osc_server, udp_client\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def loop_gen(unused_addr, filepath, duration, bpm, temperature=1.0, description=\"funky beat\"):\n",
    "    print(filepath)\n",
    "    filepath = filepath.replace(\"C:/\", \"/mnt/c/\") # file path is in windows format\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        outpaths = _loop_gen(filepath, duration, bpm, temperature, description)\n",
    "        # if outpath is not None:    \n",
    "        client.send_message(\"/generated\", (1))\n",
    "        client.send_message(\"/generated\", (1))\n",
    "    else:\n",
    "        print(\"file not found\", filepath)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "dispatcher = dispatcher.Dispatcher()\n",
    "dispatcher.map(\"/loop_gen\", loop_gen)\n",
    "\n",
    "#%%\n",
    "\n",
    "#client = udp_client.SimpleUDPClient('127.0.0.1', 10018)\n",
    "client = udp_client.SimpleUDPClient('172.17.128.1', 10018)\n",
    "\n",
    "\n",
    "def beacon():\n",
    "    while True:\n",
    "        client.send_message(\"/heartbeat\", 1)\n",
    "        time.sleep(1.0)\n",
    "t1 = Thread(target = beacon)\n",
    "t1.setDaemon(True)\n",
    "t1.start()\n",
    "\n",
    "server = osc_server.ThreadingOSCUDPServer(\n",
    "    ('172.17.140.208', 10026), dispatcher)\n",
    "print(\"Serving on {}\".format(server.server_address))\n",
    "server.serve_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "b02c911f9b3627d505ea4a19966a915ef21f28afb50dbf6b2115072d27c69103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
