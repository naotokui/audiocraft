{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MusicGen\n",
    "Welcome to MusicGen's demo jupyter notebook. Here you will find a series of self-contained examples of how to use MusicGen in different settings.\n",
    "\n",
    "First, we start by initializing MusicGen, you can choose a model from the following selection:\n",
    "1. `facebook/musicgen-small` - 300M transformer decoder.\n",
    "2. `facebook/musicgen-medium` - 1.5B transformer decoder.\n",
    "3. `facebook/musicgen-melody` - 1.5B transformer decoder also supporting melody conditioning.\n",
    "4. `facebook/musicgen-large` - 3.3B transformer decoder.\n",
    "\n",
    "We will use the `facebook/musicgen-small` variant for the purpose of this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 4044738e-4ec9-4c9b-8ce0-52a425e70ce5)')' thrown while requesting HEAD https://huggingface.co/facebook/musicgen-small/resolve/main/state_dict.bin\n"
     ]
    }
   ],
   "source": [
    "from audiocraft.models import MusicGen\n",
    "#from audiocraft.models import MultiBandDiffusion\n",
    "import torch, torchaudio\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "from audiocraft.data.audio import audio_write\n",
    "\n",
    "USE_DIFFUSION_DECODER = False\n",
    "# Using small model, better results would be obtained with `medium` or `large`.\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "#if USE_DIFFUSION_DECODER:\n",
    "#    mbd = MultiBandDiffusion.get_mbd_musicgen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us configure the generation parameters. Specifically, you can control the following:\n",
    "* `use_sampling` (bool, optional): use sampling if True, else do argmax decoding. Defaults to True.\n",
    "* `top_k` (int, optional): top_k used for sampling. Defaults to 250.\n",
    "* `top_p` (float, optional): top_p used for sampling, when set to 0 top_k is used. Defaults to 0.0.\n",
    "* `temperature` (float, optional): softmax temperature parameter. Defaults to 1.0.\n",
    "* `duration` (float, optional): duration of the generated waveform. Defaults to 30.0.\n",
    "* `cfg_coef` (float, optional): coefficient used for classifier free guidance. Defaults to 3.0.\n",
    "\n",
    "When left unchanged, MusicGen will revert to its default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_duration = 12\n",
    "\n",
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=output_duration,\n",
    "    temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can go ahead and start generating music using one of the following modes:\n",
    "* Unconditional samples using `model.generate_unconditional`\n",
    "* Music continuation using `model.generate_continuation`\n",
    "* Text-conditional samples using `model.generate`\n",
    "* Melody-conditional samples using `model.generate_with_chroma`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_fade(audio, sr, duration=.1):\n",
    "    # convert to audio indices (samples)\n",
    "    length = int(duration*sr)\n",
    "    end = audio.shape[0]\n",
    "    start = end - length\n",
    "\n",
    "    # compute fade out curve\n",
    "    # linear fade\n",
    "    fade_curve = np.linspace(1.0, 0.0, length)\n",
    "\n",
    "    # apply the curve\n",
    "    audio[start:end] = audio[start:end] * fade_curve\n",
    "\n",
    "    fade_curve = np.linspace(0.0, 1.0, length)\n",
    "\n",
    "    # apply the curve\n",
    "    audio[:length] = audio[:length] * fade_curve\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from madmom.features.downbeats import DBNDownBeatTrackingProcessor\n",
    "from madmom.features.downbeats import RNNDownBeatProcessor\n",
    "import os\n",
    "import soundfile as sf\n",
    "import pyrubberband as pyrb\n",
    "import librosa\n",
    "\n",
    "proc = DBNDownBeatTrackingProcessor(beats_per_bar=4, fps = 100, verbose=False)\n",
    "\n",
    "def extract_loop(file_path, desired_bpm, num_bars=2):\n",
    "    try:\n",
    "        _, sr = librosa.core.load(file_path, sr=None) # sr = None will retrieve the original sampling rate = 44100\n",
    "    except:\n",
    "        print('load file failed')\n",
    "        return None\n",
    "    try:\n",
    "        act = RNNDownBeatProcessor(verbose=False)(file_path)\n",
    "        down_beat=proc(act, verbose=False) # [..., 2] 2d-shape numpy array\n",
    "    except Exception as exp:\n",
    "        print('except happended', exp)\n",
    "        return None\n",
    "    count = 0\n",
    "    name = file_path.replace('.wav', '')\n",
    "    for i in range(8, down_beat.shape[0]):\n",
    "        if down_beat[i][1] == 1 and i + 4*num_bars < down_beat.shape[0] and down_beat[i+4*num_bars][1] == 1:\n",
    "            start_time = down_beat[i][0]\n",
    "            end_time = down_beat[i + 4*num_bars][0]\n",
    "            count += 1\n",
    "            out_path = os.path.join(\"./\", f'{name}_{count}.wav')\n",
    "            y_one_bar, _ = librosa.core.load(file_path, offset=start_time, duration = end_time - start_time, sr=None)\n",
    "            desired_duration = 60./desired_bpm * (4*num_bars)\n",
    "            y_stretch = pyrb.time_stretch(y_one_bar, sr,  (end_time - start_time) / desired_duration)\n",
    "            y_stretch = apply_fade(y_stretch, sr)\n",
    "            sf.write(out_path, y_stretch, sr)\n",
    "            return out_path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You can also use any audio from a file. Make sure to trim the file if it is too long!\n",
    "# from audiocraft.data.audio import audio_write\n",
    "\n",
    "# prompt_waveform, prompt_sr = torchaudio.load(\"../assets/beat.wav\")\n",
    "# prompt_duration = 4\n",
    "# print(prompt_waveform.shape)\n",
    "\n",
    "# prompt_waveform = prompt_waveform[..., :int(prompt_duration * prompt_sr)]\n",
    "# prompt_waveform = prompt_waveform[None, ...].repeat(3, 1, 1)\n",
    "\n",
    "# output = model.generate_continuation(prompt_waveform, prompt_sample_rate=prompt_sr, \n",
    "#                                      descriptions=[\"reggae music on the tropical beach\", \"evocative jazz music with women voice singing emotionally like Bjork\", \"funky beats\"], progress=True, return_tokens=True)\n",
    "# print(output[0].shape)\n",
    "# sr = model.sample_rate\n",
    "# start_pos = prompt_duration * sr \n",
    "# display_audio(output[0], sample_rate=32000)\n",
    "\n",
    "# for index in range(output[0].shape[0]):\n",
    "#     file_path = \"output\"\n",
    "#     output_data = output[0][index].cpu().squeeze()\n",
    "#     audio_write(file_path, output_data, sr, strategy=\"loudness\", loudness_compressor=True)\n",
    "#     #print(output_data.shape)\n",
    "#     #sf.write(file_path, output_data, sr)\n",
    "#     #extract_loop(file_path + \".wav\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_fadeout(audio, sr, duration=.1):\n",
    "    # convert to audio indices (samples)\n",
    "    length = int(duration*sr)\n",
    "    end = audio.shape[0]\n",
    "    start = end - length\n",
    "\n",
    "    # compute fade out curve\n",
    "    # linear fade\n",
    "    fade_curve = np.linspace(1.0, 0.0, length)\n",
    "\n",
    "    # apply the curve\n",
    "    audio[start:end] = audio[start:end] * fade_curve\n",
    "\n",
    "    fade_curve = np.linspace(0.0, 1.0, length)\n",
    "\n",
    "    # apply the curve\n",
    "    audio[:length] = audio[:length] * fade_curve\n",
    "\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "\n",
    "def _loop_gen(filepath, duration, bpm, temperature=1.0, description=\"jazzy beat\"):\n",
    "\n",
    "    model.set_generation_params(\n",
    "        use_sampling=True,\n",
    "        top_k=250,\n",
    "        duration=output_duration,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    print(\"description:\", description, \"duration\", duration)\n",
    "    prompt_waveform, prompt_sr = torchaudio.load(filepath)\n",
    "    prompt_duration = duration/1000.\n",
    "    prompt_waveform = prompt_waveform[..., :int(prompt_duration * prompt_sr)]\n",
    "    prompt_waveform = prompt_waveform[None, ...].repeat(batch_size, 1, 1)\n",
    "\n",
    "    descriptions = [description] * batch_size\n",
    "    output = model.generate_continuation(prompt_waveform, prompt_sample_rate=prompt_sr, descriptions=descriptions, \n",
    "                                         progress=True, return_tokens=True)\n",
    "    print(output[0].shape)\n",
    "    sr = model.sample_rate\n",
    "    start_pos = int(prompt_duration * sr) * 0 \n",
    "    display_audio(output[0][:,:,start_pos:], sample_rate=32000)\n",
    "\n",
    "    outpaths = []\n",
    "    for index in range(batch_size):\n",
    "        file_path = \"/mnt/c/tmp/output%d\" % index\n",
    "        start_pos = 0\n",
    "        output_data = output[0][index].cpu().squeeze()[start_pos:]\n",
    "        audio_write(file_path, output_data, sr, strategy=\"loudness\", loudness_compressor=True)\n",
    "        outpath = extract_loop(file_path + \".wav\", bpm, num_bars=2)\n",
    "        outpaths.append(outpath)\n",
    "    return outpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/tmp/input.wav\n",
      "description: IDM complex beat duration 3953\n",
      "/mnt/c/tmp/input.wav\n",
      "description: IDM complex beat duration 3953\n",
      "    24 /   1000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('172.17.128.1', 50944)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/socketserver.py\", line 683, in process_request_thread\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/pythonosc/osc_server.py\", line 33, in handle\n",
      "    server.dispatcher.call_handlers_for_packet(self.request[0], self.client_address)\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/pythonosc/dispatcher.py\", line 193, in call_handlers_for_packet\n",
      "    handler.invoke(client_address, timed_msg.message)\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/pythonosc/dispatcher.py\", line 56, in invoke\n",
      "    self.callback(message.address, *message)\n",
      "  File \"/tmp/ipykernel_397/18749682.py\", line 13, in loop_gen\n",
      "    outpaths = _loop_gen(filepath, duration, bpm, temperature, description)\n",
      "  File \"/tmp/ipykernel_397/3194508071.py\", line 19, in _loop_gen\n",
      "    output = model.generate_continuation(prompt_waveform, prompt_sample_rate=prompt_sr, descriptions=descriptions,\n",
      "  File \"/mnt/c/Users/nao/Documents/GitHub/audiocraft/audiocraft/models/musicgen.py\", line 251, in generate_continuation\n",
      "    tokens = self._generate_tokens(attributes, prompt_tokens, progress)\n",
      "  File \"/mnt/c/Users/nao/Documents/GitHub/audiocraft/audiocraft/models/musicgen.py\", line 349, in _generate_tokens\n",
      "    gen_tokens = self.lm.generate(\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mnt/c/Users/nao/Documents/GitHub/audiocraft/audiocraft/models/lm.py\", line 494, in generate\n",
      "    next_token = self._sample_next_token(\n",
      "  File \"/mnt/c/Users/nao/Documents/GitHub/audiocraft/audiocraft/models/lm.py\", line 354, in _sample_next_token\n",
      "    all_logits = model(\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/c/Users/nao/Documents/GitHub/audiocraft/audiocraft/models/lm.py\", line 253, in forward\n",
      "    out = self.transformer(input_, cross_attention_src=cross_attention_input)\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/c/Users/nao/Documents/GitHub/audiocraft/audiocraft/modules/transformer.py\", line 700, in forward\n",
      "    x = self._apply_layer(layer, x, *args, **kwargs)\n",
      "  File \"/mnt/c/Users/nao/Documents/GitHub/audiocraft/audiocraft/modules/transformer.py\", line 657, in _apply_layer\n",
      "    return layer(*args, **kwargs)\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/c/Users/nao/Documents/GitHub/audiocraft/audiocraft/modules/transformer.py\", line 551, in forward\n",
      "    self._sa_block(self.norm1(x), src_mask, src_key_padding_mask))\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 599, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "  File \"/home/nao/anaconda3/envs/mgen/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/c/Users/nao/Documents/GitHub/audiocraft/audiocraft/modules/transformer.py\", line 337, in forward\n",
      "    attn_mask = self._get_mask(query.shape[1], query.device, query.dtype)\n",
      "  File \"/mnt/c/Users/nao/Documents/GitHub/audiocraft/audiocraft/modules/transformer.py\", line 244, in _get_mask\n",
      "    raise RuntimeError(\"Not supported at the moment\")\n",
      "RuntimeError: Not supported at the moment\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   564 /   1000\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "import os \n",
    "import time\n",
    "from threading import Thread\n",
    "from pythonosc import dispatcher\n",
    "from pythonosc import osc_server, udp_client\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def loop_gen(unused_addr, filepath, duration, bpm, temperature=1.0, description=\"funky beat\"):\n",
    "    print(filepath)\n",
    "    if os.path.exists(filepath):\n",
    "        outpaths = _loop_gen(filepath, duration, bpm, temperature, description)\n",
    "        # if outpath is not None:    \n",
    "        client.send_message(\"/generated\", (1))\n",
    "        client.send_message(\"/generated\", (1))\n",
    "    else:\n",
    "        print(\"file not found\", filepath)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "dispatcher = dispatcher.Dispatcher()\n",
    "dispatcher.map(\"/loop_gen\", loop_gen)\n",
    "\n",
    "#%%\n",
    "\n",
    "#client = udp_client.SimpleUDPClient('127.0.0.1', 10018)\n",
    "client = udp_client.SimpleUDPClient('172.17.128.1', 10018)\n",
    "\n",
    "\n",
    "def beacon():\n",
    "    while True:\n",
    "        client.send_message(\"/heartbeat\", 1)\n",
    "        time.sleep(1.0)\n",
    "t1 = Thread(target = beacon)\n",
    "t1.setDaemon(True)\n",
    "t1.start()\n",
    "\n",
    "server = osc_server.ThreadingOSCUDPServer(\n",
    "    ('172.17.140.208', 10026), dispatcher)\n",
    "print(\"Serving on {}\".format(server.server_address))\n",
    "server.serve_forever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "b02c911f9b3627d505ea4a19966a915ef21f28afb50dbf6b2115072d27c69103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
